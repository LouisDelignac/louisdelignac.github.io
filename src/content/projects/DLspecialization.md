### Course summary
###### Course followed from December 3, 2025 to January 16, 2026

The [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning) by DeepLearning.AI is taught by [Andrew Ng](https://en.wikipedia.org/wiki/Andrew_Ng). It is a benchmark course on neural networks that I completed independently while looking for a job. Following my Master's in Statistical Modeling, my goal was to dive deeper into this field I'm passionate about and get into the details of specific technologies like CNNs and RNNs.

During this approximately 125-hour course, which combines video lectures, quizzes, and programming assignments, I was able to:

- Implement various complex architectures (YOLO, LSTMs, Transformers, etc.).
- Apply best practices for managing Deep Learning projects.
- Solve concrete use cases (object detection, chatbots, speech recognition, etc.).

This training was a truly enriching experience. I genuinely enjoyed getting up in the morning to watch the lectures, Andrew is an excellent instructor!

ðŸ”— [**View Certification**](https://coursera.org/verify/specialization/EMTN73QSOL83)


### Course outline

#### Course 1: Neural Networks and Deep Learning

This foundational course covers how to build, train, and apply deep neural networks.

- **Introduction to Deep Learning:** Major trends driving the rise of Deep Learning.
- **Neural Network Basics:** Logistic Regression as a Neural Network, Gradient Descent, Vectorization (Python/NumPy).
- **Shallow Neural Networks:** Hidden layers, Activation functions (Sigmoid, Tanh, ReLU), Random initialization.
- **Deep Neural Networks:** Forward and Backward propagation, Parameters vs. Hyperparameters, Building deep L-layer networks.

#### Course 2: Improving Deep Neural Networks: Hyperparameter Tuning, Regularization, and Optimization

This course focuses on the "practical toolbox" needed to make models work efficiently and train quickly.

- **Practical Aspects of Deep Learning:** Train/Dev/Test sets, Bias and Variance, Regularization (L2, Dropout), Data normalization, Vanishing/Exploding gradients.
- **Optimization Algorithms:** Mini-batch gradient descent, Exponentially weighted averages, Momentum, RMSprop, Adam optimizer.
- **Hyperparameter Tuning & Batch Normalization:** Tuning strategies (grid vs. random), Batch Normalization, Introduction to Deep Learning frameworks (TensorFlow).

#### Course 3: Structuring Machine Learning Projects

This is a unique, strategy-focused course based on industry experience. It teaches you how to make technical decisions to save time and improve performance.

- **ML Strategy:** Orthogonalization, Single number evaluation metric, Satisficing and Optimizing metrics.
- **Comparing to Human-level Performance:** Avoidable bias, Understanding human-level performance.
- **Error Analysis:** Cleaning up incorrectly labeled data, when to change the metric.
- **Mismatched Training and Dev/Test Sets:** Data mismatch problems, Transfer Learning, Multi-task Learning.
- **End-to-end Deep Learning:** Pros and cons of end-to-end approaches.

#### Course 4: Convolutional Neural Networks

This course is dedicated to Computer Vision.

- **Foundations of Convolutional Neural Networks:** Convolution operation, Padding, Stride, Pooling layers.
- **Deep Convolutional Models:** Case studies of classic networks (LeNet-5, AlexNet, VGG), ResNets (Residual Networks), Inception Network.
- **Object Detection:** Localization, Landmark detection, YOLO (You Only Look Once) algorithm, Intersection over Union, Non-max suppression, Anchor boxes.
- **Face Recognition & Neural Style Transfer:** Siamese Networks, Triplet Loss, Face verification vs. recognition, Art generation with Neural Style Transfer.

#### Course 5: Sequence Models

The final course covers sequential data such as natural language (NLP), audio, and time series.

- **Recurrent Neural Networks:** Sequence models, Backpropagation through time, Vanishing gradients in RNNs (Recurrent Neural Networks), GRU (Gated Recurrent Unit), LSTM (Long Short-Term Memory).
- **Natural Language Processing & Word Embeddings:** Word representation, Word2Vec, GloVe, Negative Sampling, Debiasing word embeddings.
- **Sequence-to-Sequence Models:** Beam Search, Bleu Score, Attention Model.
- **Transformers:** The Transformer Network architecture, Self-Attention, Multi-Head Attention (the foundation of modern LLMs like GPT and BERT).